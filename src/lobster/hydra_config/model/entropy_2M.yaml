_target_: lobster.model.LobsterPCLM

lr: 1e-3
beta1: 0.9
beta2: 0.98
eps: 1e-12
num_warmup_steps: 1000
model_name: CLM_bottleneck
ckpt_path: null
tokenizer_dir: null
max_length: 512
num_training_steps: ${trainer.max_steps}
num_key_value_heads: null
attention_bias: false

# Model-specific configuration parameters
model_kwargs:
  embedding_layer: linear_pos 
  hidden_act: gelu


num_training_steps: ${trainer.max_steps}

scheduler: "cosine_with_min_lr"
# Scheduler-specific configuration parameters
scheduler_kwargs:
  min_lr: 1e-7